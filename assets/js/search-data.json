{
  
    
        "post0": {
            "title": "Out-of-distribution detection with Plex",
            "content": "Today we describe a framework for reliable deep learning, which includes tasks and datasets to stress-test model reliability. We also introduce Plex, a set of pre-trained large model extensions that can be applied to architectures to improve reliability. https://t.co/acXkGApnLy pic.twitter.com/WMJFCA6qqV . &mdash; Google AI (@GoogleAI) July 14, 2022 . This notebook demonstrates how one can utilize the released checkpoints from the Plex: Towards Reliability using Pretrained Large Model Extensions paper which are using JAX and Tensorflow as backends. The General usage section provides a minimal setup for loading the checkpoints. Plex is designed to excel in three areas of reliability, Uncertainty, Robust Generalization, and Adaptation. Here, we focus at the Adaptation aspect of pretrained models by demonstrating zero-shot out-of-distribution (OOD) detection. . Imports . import functools from clu import preprocess_spec import flax import jax import jax.numpy as jnp import numpy as np import matplotlib.pyplot as plt import ml_collections import sklearn import tensorflow as tf import tensorflow_datasets as tfds import uncertainty_baselines as ub import checkpoint_utils # local file import from Plex repository import input_utils # local file import from Plex repository import preprocess_utils # local file import from Plex repository . /home/users/yugam/anaconda3/envs/drpy9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm WARNING:absl:Skipped importing the SMCalflow dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras. Traceback (most recent call last): File &#34;/home/users/yugam/uncertainty-baselines/uncertainty_baselines/datasets/datasets.py&#34;, line 60, in &lt;module&gt; from uncertainty_baselines.datasets.smcalflow import MultiWoZDataset # pylint: disable=g-import-not-at-top File &#34;/home/users/yugam/uncertainty-baselines/uncertainty_baselines/datasets/smcalflow.py&#34;, line 40, in &lt;module&gt; import seqio ModuleNotFoundError: No module named &#39;seqio&#39; WARNING:absl:Skipped importing the SMCalflow dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras. Traceback (most recent call last): File &#34;/home/users/yugam/uncertainty-baselines/uncertainty_baselines/datasets/__init__.py&#34;, line 71, in &lt;module&gt; from uncertainty_baselines.datasets.smcalflow import MultiWoZDataset # pylint: disable=g-import-not-at-top File &#34;/home/users/yugam/uncertainty-baselines/uncertainty_baselines/datasets/smcalflow.py&#34;, line 40, in &lt;module&gt; import seqio ModuleNotFoundError: No module named &#39;seqio&#39; . . # Set a base seed to use for the notebook. rng = jax.random.PRNGKey(42) print(jax.local_devices()) . 2022-09-14 15:22:53.573404: W external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcuda.so.1&#39;; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/users/yugam/anaconda3/envs/drpy9/lib/ WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 2022-09-14 15:22:53.573448: W external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303) . [CpuDevice(id=0)] . . General Usage . Load Pretrained ViT-Plex model . def get_pretrained_config(): # From `https://github.com/google/uncertainty-baselines/blob/main/baselines/jft/experiments/vit_be/imagenet21k_be_vit_large_32.py`. config = ml_collections.ConfigDict() config.model = ml_collections.ConfigDict() config.model.patches = ml_collections.ConfigDict() config.model.patches.size = [32, 32] config.model.hidden_size = 1024 config.model.transformer = ml_collections.ConfigDict() config.model.transformer.mlp_dim = 4096 config.model.transformer.num_heads = 16 config.model.transformer.num_layers = 24 config.model.transformer.attention_dropout_rate = 0. config.model.transformer.dropout_rate = 0.1 config.model.classifier = &#39;token&#39; config.model.representation_size = 1024 # BatchEnsemble config.model.transformer.be_layers = (21, 22, 23) config.model.transformer.ens_size = 3 config.model.transformer.random_sign_init = -0.5 config.model.transformer.ensemble_attention = False return config . num_classes = 21843 config = get_pretrained_config() model = ub.models.vision_transformer_be( num_classes=num_classes, **config.model) . /home/users/yugam/anaconda3/envs/drpy9/lib/python3.9/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead. PyTreeDef = type(jax.tree_structure(None)) WARNING:absl:Skipped BERT models due to ImportError. Traceback (most recent call last): File &#34;/home/users/yugam/uncertainty-baselines/uncertainty_baselines/models/__init__.py&#34;, line 105, in &lt;module&gt; from uncertainty_baselines.models import bert File &#34;/home/users/yugam/uncertainty-baselines/uncertainty_baselines/models/bert.py&#34;, line 30, in &lt;module&gt; from official.nlp.bert import bert_models ModuleNotFoundError: No module named &#39;official.nlp.bert&#39; . . checkpoint_path = &#39;gs://plex-paper/plex_vit_large_imagenet21k.npz&#39; read_in_parallel = False checkpoint = checkpoint_utils.load_checkpoint(None, path=checkpoint_path, read_in_parallel=read_in_parallel) pretrained_params = checkpoint[&quot;opt&quot;][&quot;target&quot;] . 2022-09-14 15:23:03.436479: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with &#34;NOT_FOUND: Could not locate the credentials file.&#34;. Retrieving token from GCE failed with &#34;FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning &#39;Couldn&#39;t resolve host name&#39;, error details: Could not resolve host: metadata&#34;. . . Zero-shot out-of-distribution detection . Here we demonstrate zero-shot out-of-distribution (OOD) detection using the upstream pretrained model and the Relative Mahalanobis distance metric (Ren et al., 2021). In zero-shot OOD detection, the goal is to take a fixed model that was pretrained on dataset A and use it to distinguish between in-distributions samples from dataset B and OOD sample from dataset C, all without fine-tuning the model further on datset B or C. We see that pretrained Plex without any finetuning is able to achieve a strong separation between in and out of distribution. . Extracting embeddings/representations from the pretrained model . @jax.jit def representation_fn(params, images, rng): rng_dropout, rng_diag_noise, rng_standard_noise = jax.random.split(rng, num=3) _, out = model.apply( {&#39;params&#39;: flax.core.freeze(params)}, images, train=False, rngs={ &#39;dropout&#39;: rng_dropout, &#39;diag_noise_samples&#39;: rng_diag_noise, &#39;standard_norm_noise_samples&#39;: rng_standard_noise}) representations = out[&quot;pre_logits&quot;] ens_representations = jnp.stack(jnp.split(representations, model.transformer.ens_size), axis=1) return ens_representations # Shape (batch_size, ens_sizen, um_classes). . pmapped_representation_fn = jax.pmap(representation_fn, in_axes=(None, 0, None)) # Create a model function that works across multiple TPU devices or across # multiple GPUs for performance. The value for `in_axes` means that the `params` # argument for `representation_fn` will be copied to each device, the `images` will be # split (&quot;sharded&quot;) across the devices along the first axis, and the `rng` will # be copied to each device. Note that this means that `images` should have shape # `(num_devices, batch_size/num_devices, h, w, c)` so that each device processes # a `(batch_size/num_devices, h, w, c)` chunk of the images. The `params` and # `rng` will be the same as in the &quot;Singe image&quot; example up above. . Load In Distribution Dataset . def load_val_ds(dataset, split, batch_size, preprocess_eval_fn): # NOTE: The data loader yields examples of shape # (num_devices, batch_size/num_devices, ...), i.e., it splits the batch_size # across the number of local devices, under the assumption that TPUs or # multiple GPUs are used. val_ds = input_utils.get_data( dataset=dataset, split=split, rng=None, process_batch_size=batch_size, preprocess_fn=preprocess_eval_fn, cache=False, num_epochs=1, repeat_after_batching=True, shuffle=False, prefetch_size=0, drop_remainder=False, data_dir=None) return val_ds . # https://www.tensorflow.org/datasets/catalog/imagenet_v2 dataset = &quot;imagenet_v2&quot; tfds.builder(dataset).download_and_prepare() batch_size = 64 * jax.local_device_count() split = &quot;test&quot; in_ds, in_ds_info = tfds.load(dataset, split=split, with_info=True) pp_eval = f&quot;decode|resize_small(256)|central_crop(224)|value_range(-1, 1)|onehot(1000, key=&#39;label&#39;, key_result=&#39;labels&#39;)|keep([&#39;image&#39;, &#39;labels&#39;])&quot; preprocess_eval_fn = preprocess_spec.parse( spec=pp_eval, available_ops=preprocess_utils.all_ops()) in_val_ds = load_val_ds(dataset, split=split, batch_size=batch_size, preprocess_eval_fn=preprocess_eval_fn) . 2022-09-14 15:32:21.365371: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcuda.so.1&#39;; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/users/yugam/anaconda3/envs/drpy9/lib/ 2022-09-14 15:32:21.365436: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead. WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead. . . fig = tfds.show_examples(in_ds, in_ds_info) . . Load Out of Distribution Dataset . # https://www.tensorflow.org/datasets/catalog/fashion_mnist dataset = &quot;fashion_mnist&quot; tfds.builder(dataset).download_and_prepare() batch_size = 64 * jax.local_device_count() split = &quot;test&quot; out_ds, out_ds_info = tfds.load(dataset, split=split, with_info=True) pp_eval = &quot;decode|resize_small(256)|central_crop(224)|value_range(-1, 1)|keep([&#39;image&#39;])&quot; preprocess_eval_fn = preprocess_spec.parse( spec=pp_eval, available_ops=preprocess_utils.all_ops()) out_val_ds = load_val_ds(dataset, split=split, batch_size=batch_size, preprocess_eval_fn=preprocess_eval_fn) . WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead. WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead. . . fig_out = tfds.show_examples(out_ds, out_ds_info) . 2022-09-14 15:32:25.513154: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . . Computing Uncertainty Score : Relative Mahalanobis Distance . @jax.jit def compute_mean_and_cov(embeds, labels, class_ids): &quot;&quot;&quot;Computes class-specific means and a shared covariance matrix. Args: embeds: A jnp.array of size [n_train_sample, n_dim], where n_train_sample is the sample size of training set, n_dim is the dimension of the embedding. labels: A jnp.array of size [n_train_sample, ]. class_ids: A jnp.array of the unique class ids in `labels`. Returns: means: A list of len n_class, and the i-th element is an np.array of size [n_dim, ] corresponding to the mean of the fitted Gaussian distribution for the i-th class. cov: The shared covariance matrix of the size [n_dim, n_dim]. &quot;&quot;&quot; n_dim = embeds.shape[1] cov = jnp.zeros((n_dim, n_dim)) def f(cov, class_id): mask = jnp.expand_dims(labels == class_id, axis=-1) # to compute mean/variance use only those which belong to current class_id data = embeds * mask mean = jnp.sum(data, axis=0) / jnp.sum(mask) diff = (data - mean) * mask cov += jnp.matmul(diff.T, diff) return cov, mean cov, means = jax.lax.scan(f, cov, class_ids) cov = cov / len(labels) return means, cov . @jax.jit def compute_mahalanobis_distance(embeds, means, cov): &quot;&quot;&quot;Computes Mahalanobis distance between the input and the fitted Guassians. The Mahalanobis distance (Mahalanobis, 1936) is defined as `distance(x, mu, sigma) = sqrt((x- mu)^T sigma^{-1} (x- mu))`, where `x` is a vector, `mu` is the mean vector for a Gaussian, and `sigma` is the covariance matrix. We compute the distance for all examples in `embeds`, and across all classes in `means`. Note that this function technically computes the squared Mahalanobis distance, which is consistent with Eq.(2) in &lt;TODO&gt;. Args: embeds: A matrix size [n_test_sample, n_dim], where n_test_sample is the sample size of the test set, and n_dim is the size of the embeddings. means: A matrix of size [num_classes, n_dim], where the ith row corresponds to the mean of the fitted Gaussian distribution for the i-th class. cov: The shared covariance mmatrix of the size [n_dim, n_dim]. Returns: A matrix of size [n_test_sample, n_class] where the [i, j] element corresponds to the Mahalanobis distance between i-th sample to the j-th class Gaussian. &quot;&quot;&quot; # NOTE: It&#39;s possible for `cov` to be singular, in part because it is # estimated on a sample of data. This can be exacerbated by lower precision, # where, for example, the matrix could be non-singular in float64, but # singular in float32. For our purposes in computing Mahalanobis distance, # using a pseudoinverse is a reasonable approach that will be equivalent to # the inverse if `cov` is non-singular. cov_inv = jnp.linalg.pinv(cov) def maha_dist(x, mean): # NOTE: This computes the squared Mahalanobis distance. diff = x - mean return jnp.einsum(&quot;i,ij,j-&gt;&quot;, diff, cov_inv, diff) # Vectorize over all classes means, and map in a fast loop over examples. # Given more memory, one could vectorize over examples as well. maha_dist_all_classes_fn = jax.vmap(maha_dist, in_axes=(None, 0)) out = jax.lax.map(lambda x: maha_dist_all_classes_fn(x, means), embeds) return out . def get_and_reshape(x): # Fetch probs from all devices to CPU and reshape to (batch_size, ...). return jnp.reshape(jax.device_get(x), (-1,) + x.shape[2:]) . in_dist_representations = [] in_dist_labels = [] masks = [] in_val_ds = in_val_ds.shuffle(256, seed=42).take(int(1024 / batch_size)) for i, batch in enumerate(in_val_ds.as_numpy_iterator()): rng_eval = jax.random.fold_in(rng, 0) representation = pmapped_representation_fn(pretrained_params, batch[&quot;image&quot;], rng_eval) in_dist_representations.append(get_and_reshape(representation)) masks.append(get_and_reshape(batch[&quot;mask&quot;])) in_dist_labels.append(get_and_reshape(jnp.argmax(batch[&quot;labels&quot;], axis=-1))) . mask = jnp.concatenate(jax.device_get(masks)) in_dist_representations = jnp.concatenate(in_dist_representations)[mask == 1] in_dist_labels = jnp.concatenate(in_dist_labels)[mask == 1] . in_dist_representations.shape, in_dist_labels.shape . ((1024, 3, 1024), (1024,)) . . ood_representations = [] masks = [] out_val_ds = out_val_ds.shuffle(256, seed=42).take(int(1024 / batch_size)) for i, batch in enumerate(out_val_ds.as_numpy_iterator()): rng_eval = jax.random.fold_in(rng, 0) representation = pmapped_representation_fn(pretrained_params, batch[&quot;image&quot;], rng_eval) ood_representations.append(get_and_reshape(representation)) masks.append(get_and_reshape(batch[&quot;mask&quot;])) . mask = jnp.concatenate(masks) ood_representations = jnp.concatenate(ood_representations)[mask == 1] . ood_representations.shape . (1024, 3, 1024) . . ens_means, ens_covs = [], [] ens_means_background, ens_covs_background = [], [] for m in range(in_dist_representations.shape[1]): means, cov = compute_mean_and_cov(in_dist_representations[:, m], in_dist_labels, class_ids=jnp.unique(in_dist_labels)) ens_means.append(means) ens_covs.append(cov) means_bg, cov_bg = compute_mean_and_cov(in_dist_representations[:, m], jnp.zeros_like(in_dist_labels), class_ids=jnp.array([0])) ens_means_background.append(means_bg) ens_covs_background.append(cov_bg) . ens_in_dist_rmaha_distances = [] for m in range(len(ens_means)): distances = compute_mahalanobis_distance(in_dist_representations[:, m], ens_means[m], ens_covs[m]) distances_bg = compute_mahalanobis_distance(in_dist_representations[:, m], ens_means_background[m], ens_covs_background[m]) rmaha_distances = jnp.min(distances, axis=-1) - distances_bg[:, 0] ens_in_dist_rmaha_distances.append(rmaha_distances) in_dist_rmaha_distances = jnp.mean(jnp.array(ens_in_dist_rmaha_distances), axis=0) del ens_in_dist_rmaha_distances . print((in_dist_rmaha_distances, in_dist_rmaha_distances.shape)) . (DeviceArray([ 159.8851 , -552.7504 , -173.4978 , ..., -733.5099 , -299.4077 , -816.06104], dtype=float32), (1024,)) . . ens_ood_rmaha_distances = [] for m in range(len(ens_means)): distances = compute_mahalanobis_distance(ood_representations[:, m], ens_means[m], ens_covs[m]) distances_bg = compute_mahalanobis_distance(ood_representations[:, m], ens_means_background[m], ens_covs_background[m]) rmaha_distances = jnp.min(distances, axis=-1) - distances_bg[:, 0] ens_ood_rmaha_distances.append(rmaha_distances) ood_rmaha_distances = jnp.mean(jnp.array(ens_ood_rmaha_distances), axis=0) del ens_ood_rmaha_distances . print((ood_rmaha_distances, ood_rmaha_distances.shape)) . (DeviceArray([ 983.19275, 964.8354 , 587.03705, ..., 1065.6013 , 1109.4915 , 1084.283 ], dtype=float32), (1024,)) . . Visualising uncertainty scores . plt.hist([in_dist_rmaha_distances, ood_rmaha_distances], bins=100, density=True, label=[&quot;in-dist&quot;, &quot;ood&quot;]) plt.legend() plt.show() . .",
            "url": "https://yugamt.github.io/effective-ml/jupyter/2022/09/16/OOD-detection-with-Plex.html",
            "relUrl": "/jupyter/2022/09/16/OOD-detection-with-Plex.html",
            "date": " • Sep 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://yugamt.github.io/effective-ml/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://yugamt.github.io/effective-ml/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello there! I am Yugam Tiwari, currently a Associate Machine Learning Scientist at Wadhwani AI. .",
          "url": "https://yugamt.github.io/effective-ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yugamt.github.io/effective-ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}