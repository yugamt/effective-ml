<h1 id="testing-in-ml">Testing in ML</h1>

<h2 id="why-should-we-test">Why Should We Test?</h2>

<ul>
  <li>Easy to add new features</li>
  <li>To avoid a wasted training job.</li>
  <li>Better to find shortcomings on your own than from the clients.</li>
</ul>

<h2 id="what-to-test">What to Test?</h2>

<ol>
  <li>
    <p>Pre-train Tests
 a. Testing the code and the data
 b. Similar to software testing and the same principles can be used.</p>
  </li>
  <li>
    <p>Post-train Tests
 a. Testing the model (testing model’s skills)
 b. Well tested code and data are not sufficient to ensure the creation of a good model.
     i. Evaluation of validation set
     ii. Testing the model for some expected behaviour.
 c. These tests contain a set of inputs on which model’s performance is calculated.</p>
  </li>
</ol>

<h2 id="pre-train-tests">Pre-train Tests</h2>

<ul>
  <li>Testing the code and the data</li>
  <li>Similar to software testing and the same principles can be used.</li>
  <li>Types:
a. Unit tests:
    <ul>
      <li>To test small components of the code and the data.</li>
      <li>Any new function/class added, should have unit test testing it’s functionality.
b. Regression tests:</li>
      <li>To test bugs which have been encountered before and fixed.</li>
      <li>Can be added as a new unit test.
c. Integration tests:</li>
      <li>To test execution of multiple small components of the code.</li>
      <li>(find this out)</li>
    </ul>
  </li>
</ul>

<h2 id="unit-tests-in-ml">Unit Tests in ML</h2>
<ul>
  <li>Data
    <ul>
      <li>Check for Nan values at unexpected places.</li>
      <li>Check for leakage in your splits (by intersection).</li>
      <li>Check if features values are in the expected range. Similarly for categories.</li>
    </ul>
  </li>
  <li>DataLoader
    <ul>
      <li>Check sample’s shapes (correct or not)</li>
      <li>Check data loading (possible or not)</li>
      <li>Check augmentations (applied or not, shapes)</li>
      <li>Check everything separately for train and test samples.</li>
      <li>Ex: Augmentations differ for train and test.</li>
    </ul>
  </li>
  <li>Model
    <ul>
      <li>Check output shapes, forward pass.</li>
      <li>Check cpu to gpu to cpu transfer.</li>
      <li>Check sample independence.</li>
      <li>Check gradient’s existence for all the expected parameters.</li>
    </ul>
  </li>
  <li>Loss
    <ul>
      <li>Check output type, shape.</li>
      <li>Check expected behaviour.</li>
    </ul>
  </li>
  <li>Trainer
    <ul>
      <li>Ensure fitting (by overfitting a single batch).</li>
    </ul>
  </li>
</ul>

<h2 id="post-train-tests">Post-train Tests</h2>

<ul>
  <li>
    <p>Model evaluation on validation set</p>

    <ul>
      <li>We usually do this.</li>
      <li>Effective to compare between two models of 70% and 90% accuracy.</li>
      <li>But comparing models of 90% and 91% accuracy gets tricky.</li>
      <li>Better models could have unexpected failure modes such as bad performance on small objects, gender bias, sensitivity to low lighting conditions etc.</li>
    </ul>
  </li>
  <li>
    <p>Behavioural testing</p>
    <ul>
      <li>Manual error analysis should be done to identify failure modes of the model.</li>
      <li>Tests should be added to quantify these failure modes.</li>
      <li>These tests would be problem, domain and dataset specific.</li>
      <li>Broadly classified into three types
        <ul>
          <li>Invariance test</li>
          <li>Directional expectation test</li>
          <li>Minimum functionality test</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="invariance-test">Invariance Test</h2>

<h4 id="apply-label-preserving-perturbations-to-inputs-and-expect-model-predictions-to-remain-the-same">Apply label preserving perturbations to inputs and expect model predictions to remain the same.</h4>

<ul>
  <li>Replace places names in sentiment analysis.
    <ul>
      <li>Examples:
        <ul>
          <li>@AmericanAir thank you we got on a different flight to [ Chicago → Dallas ].</li>
          <li>@VirginAmerica I can’t lose my luggage, moving to [ Brazil → Turkey ] soon, ugh.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Typos in sentiment analysis.
    <ul>
      <li>Examples:
        <ul>
          <li>The flight was great → The flght ws great.</li>
          <li>Why are we getting so lazy → Why are we gettnig so lazy.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Rotating images in object detection.</li>
</ul>

<p><img src="/images/mobile.png" alt="" title="Farmers ensuring they get the same alert for the same trap catch." /></p>

<h2 id="directional-expectation-test">Directional Expectation Test</h2>

<h4 id="apply-perturbations-to-the-inputs-and-expect-labels-to-change-in-a-certain-way">Apply perturbations to the inputs and expect labels to change in a certain way.</h4>

<ul>
  <li>Keeping everything else the same, if house built up area is reduced, does the price increase?</li>
  <li>Adding a more negative sentence to the end should not make outputs more positive.
    <ul>
      <li>Examples:
        <ul>
          <li>@USAirways your service sucks. → @USAirways your service sucks. You are lame.</li>
          <li>@JetBlue why won’t YOU help them? → @JetBlue why won’t YOU help them? I dread you.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Replacing lower half of the image, increase the count?</li>
</ul>

<h2 id="minimum-functionality-test">Minimum Functionality Test</h2>

<h4 id="simple-test-cases-designed-to-test-a-specific-behaviour">Simple test cases designed to test a specific behaviour.</h4>

<h4 id="analogous-to-unit-test-where-we-test-the-model-on-a-small-part-of-the-data">Analogous to unit test where we test the model on a small part of the data.</h4>

<h4 id="failure-modes-discovered-in-error-analysis-can-be-tested-with-mft">Failure modes discovered in error analysis can be tested with MFT.</h4>

<ul>
  <li>Examples:
    <ul>
      <li>Testing on short, long sentences separately in sentiment analysis.</li>
      <li>Testing on small, large objects in object detection.</li>
      <li>Testing on each region’s samples separately.</li>
      <li>Testing on simple sentences created with certain template in sentiment analysis:
        <ul>
          <li>Template: “I {negation} {positive verb} the {thing}”.</li>
          <li>Sentences: I can’t say I recommend the book, I didn’t love the flight etc.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>![](/images/directional_expectation.png “”)</p>
<h2 id="minimum-functionality-test-1">Minimum Functionality Test</h2>

<ul>
  <li>Examples:
    <ul>
      <li>In pest management’s trap based solution, one of the failure modes is accepting ‘white printed paper’ as a valid trap image.</li>
    </ul>
  </li>
</ul>

<h4 id="trap-images-non-trap-images-test-case-designed-with-failure-mode-images">Trap Images^ Non-Trap Images^ Test case designed with failure mode images^</h4>

<p>![](/images/min_functionality.png “”)</p>
<h2 id="summary">Summary</h2>

<ul>
  <li>Pre-train test
    <ul>
      <li>Unit test</li>
      <li>Regression test</li>
      <li>Integration test</li>
    </ul>
  </li>
  <li>Post-train test</li>
  <li>Model evaluation on validation set</li>
  <li>Behavioural testing
    <ul>
      <li>Invariance test</li>
      <li>Directional expectation test</li>
      <li>Minimum functionality test</li>
    </ul>
  </li>
</ul>

<h2 id="final-ml-pipeline">Final ML Pipeline</h2>
<p>![](/images/final_ml.png “”)</p>

<h2 id="references">References</h2>

<ul>
  <li>Must read
    <ul>
      <li>How to Trust Your Deep Learning Code (Writing Unit Tests)</li>
      <li>Paper on Testing in NLP</li>
    </ul>
  </li>
  <li>Nice to read
    <ul>
      <li>Testing for ML Systems</li>
      <li>How to Test ML Models</li>
    </ul>
  </li>
</ul>

